{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Digital Signal Processing Courseware: An Introduction (copyright Â© 2024)\n",
    "## Authors: J. Christopher Edgar and Gregory A. Miller\n",
    "\n",
    "Conversion from Mathematica to Jupyter Notebook by Song Liu.\n",
    "\n",
    "The authors of this book are indebted to Prof. Bruce Carpenter (University of Illinois Urbana-Champaign). Bruce inspired the creation of this courseware, he consulted with the authors as this courseware was being developed, and he provided the original version of the code and text for several sections of this courseware (e.g. the section on complex numbers and the section on normal distributions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>DSP.04 Convolution and Filtering - Spatial Domain</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>Give it a Try!</font>\n",
    "# <font color=red>Part 3</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as img\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.fft import fft, fftfreq\n",
    "import matplotlib.patches as patches\n",
    "import math\n",
    "import cmath\n",
    "import pandas as pd\n",
    "from sympy import Symbol, sin, series\n",
    "from sympy import roots, solve_poly_system\n",
    "import scipy.special\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Figure size \n",
    "plt.rc(\"figure\", figsize=(8, 6))\n",
    "\n",
    "#function to create time course figure\n",
    "#one waveform\n",
    "def make_plot_1(x1,y1,type=\"b\",linewidth = 1): \n",
    "    plt.plot(x1, y1,type)\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.tick_params(labelbottom = False, bottom = False)\n",
    "    \n",
    "#two overlaid waveforms with red and blue   \n",
    "def make_plot_2(x1,y1,type1,x2,y2,type2): \n",
    "    plt.plot(x1, y1, type1)\n",
    "    plt.plot(x2, y2, type2)\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.tick_params(labelbottom = False, bottom = False)\n",
    "    \n",
    "#three overlaid waveforms with red, blue and green   \n",
    "def make_plot_3(x1,y1,type1,x2,y2,type2,x3,y3,type3): \n",
    "    plt.plot(x1, y1, type1)\n",
    "    plt.plot(x2, y2, type2)\n",
    "    plt.plot(x3, y3, type3)\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.tick_params(labelbottom = False, bottom = False)\n",
    "    \n",
    "def make_plot_3d(ax,x,y,z):    \n",
    "    ax.contour3D(x, y, z, 50, cmap=cm.coolwarm)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    \n",
    "def make_plot_freq_1(x1,sample_rate, duration=1): \n",
    "    N = sample_rate * duration\n",
    "    Nhalf = math.ceil(N/2)\n",
    "    yf = fft(x1)\n",
    "    xf = fftfreq(N, 1 / sample_rate)\n",
    "    yf = yf[0:Nhalf]\n",
    "    xf = xf[0:Nhalf]\n",
    "    plt.plot(xf, np.abs(yf))\n",
    "    \n",
    "#two spectrums\n",
    "def make_plot_freq_2(x1,x2,sample_rate, duration=1): \n",
    "    N = sample_rate * duration\n",
    "    Nhalf = math.ceil(N/2)\n",
    "    yf1 = fft(x1)\n",
    "    yf2 = fft(x2)\n",
    "    xf = fftfreq(N, 1 / sample_rate)\n",
    "\n",
    "    yf1 = yf1[0:Nhalf]\n",
    "    yf2 = yf2[0:Nhalf]\n",
    "    xf = xf[0:Nhalf]\n",
    "\n",
    "    plt.plot(xf, np.abs(yf1))\n",
    "    plt.plot(xf, np.abs(yf2), color = 'r')\n",
    "    \n",
    "def make_imshow(x):\n",
    "    plt.imshow(x,cmap='Greys_r')\n",
    "    plt.tick_params(labelbottom = False, bottom = False)\n",
    "    plt.tick_params(labelleft = False, left = False)\n",
    "    \n",
    "def make_imshow_color(x):\n",
    "    plt.imshow(x)\n",
    "    plt.tick_params(labelbottom = False, bottom = False)\n",
    "    plt.tick_params(labelleft = False, left = False)\n",
    "    \n",
    "def round_complex(x):\n",
    "    return complex(np.round(x.real,4),np.round(x.imag,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>DSP.04.G3) Using standard deviation values to identify outliers</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>DSP.04.G3.a) Calculating Z-scores and handling outliers</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = np.array([4.161803, -0.100638, 2.138029, 1.372499, -0.163366, 0.549151, 2.955684,\n",
    "-3.951537, 5.354644, -3.736839, -6.748785, -7.913515, 1.414414, -1.160518,\n",
    "-3.943281, -2.920922, -12.913827, -1.65564, 3.745551, -0.509798, 6.938785,\n",
    "0.181353, 4.817792, 8.724311, 6.437805, -2.693438, 1.133871, 5.189491, 7.243207,\n",
    "0.888545, -1.923403, -3.686132, -0.132783, -2.718729, 2.678307, 2.530582,\n",
    "-0.204171, -2.087249, 217.431945, 0.636894, -1.49119, -6.703748, -2.99446,\n",
    "0.804615, 4.96383, -3.833015, 6.271903, 0.937844, 3.010456, -0.389282, 8.077521,\n",
    "-1.422039, -3.611092, 3.295595, 5.712019, 10.466032, -1.95425, 4.452664,\n",
    "4.337543, 0.109857, 3.280643, -0.388772, 0.244246, 0.978405, -0.012601, 0.523191,\n",
    "1.891633, 2.447331, 2.876316, 1.933092, -0.773207, 7.890352, 2.865899, 1.148594,\n",
    "5.325821, 11.691833, 8.23212, 9.905125, 15.775607, 15.046681, 16.909649,\n",
    "10.737247, 5.230532, 4.704697, -5.161913, -17.312115, -28.057915, -29.840624,\n",
    "-30.954744, -37.426754, -45.539013, -41.198257, -46.638786, -51.361076,\n",
    "-50.202209, -54.358242, -54.827358, -65.26075, -64.494339, -62.19976, -67.891083,\n",
    "-64.709808, -62.695301, -63.910999, -65.830643, -63.877754, -59.0495, -53.860088,\n",
    "-50.702389, -48.609245, -46.051105, -47.389957, -44.839188, -40.158932,\n",
    "-39.228531, -35.659504, -28.532249, -30.350662, -38.398552, -28.498417,\n",
    "-27.416334, -23.32937, -24.34053, -28.01129, -32.17485, -29.345148, -32.787132,\n",
    "-31.421492, -34.017815, -38.747742, -34.092876, -36.653595, -32.668087,\n",
    "-33.902489, -32.462177, -28.102474, -33.042095, -29.791214, -23.973711,\n",
    "-17.174719, -16.891647, -20.68807, -20.423393, -16.694559, -8.641809,\n",
    "-13.102222, -10.848848, -11.987523, -12.343177, -5.389377, -9.791459, -16.60183,\n",
    "-15.996319, -10.052487, -10.610134, -10.640911, 1.584374, -214.014744, -2.376763,\n",
    "-6.416342, -2.395929, -8.566953, -4.311759, 1.170809, 0.131636, -6.57483,\n",
    "1.076944, -2.88642, -5.374261, -3.839308, -1.936249, -2.084193, -1.268859,\n",
    "5.33434, 1.509975, 4.161803, -0.100638, 2.138029, 1.372499, -0.163366, 0.549151,\n",
    "2.955684, -3.951537, 5.354644, -3.736839, -6.748785, -7.913515, 1.414414,\n",
    "-1.160518, -3.943281, -2.920922, -12.913827, -1.65564, 3.745551, -0.509798,\n",
    "6.938785, 0.181353, 4.817792, 8.724311, 6.437805, -2.693438, 1.133871, 5.189491,\n",
    "7.243207, 0.888545, -1.923403, -3.686132, -0.132783, -2.718729, -2.718729])\n",
    "\n",
    "rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rawdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 210 datapoints.\n",
    "\n",
    "In this timeseries dataset, assume that information was collected starting at -200 ms. That meanss that we chose some point in time to treat as 0 ms and that we began data collection 200 ms before that. (The reason for doing that isn't import\n",
    "ant here.) Also, assume that a sample was collected once every 4 ms. In other words, the sampling rate was 250 Hz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(-200,640, 4)\n",
    "\n",
    "# Plotting time vs amplitude using plot function from pyplot\n",
    "plt.plot(time, rawdata)\n",
    "plt.margins(x=0, y=0)\n",
    "plt.axhline(y=0, color='k')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data start at -200 ms, a datapoint is collected once every 4 ms, and there are a total of 210 datapoints.\n",
    "Modify the x axis to correctly show the time information.\n",
    "\n",
    "210 datapoints * 4 ms = 840 ms\n",
    "\n",
    "Because the start time is -200 ms, in the above figure the x axis ranges from -200 to 640 ms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several deflections from the x-axis are observed - a sharp spike at about -50 ms, a smaller, slower defection at\n",
    "about 130 ms, another similarly slow but larger and more sustained deflection at about 210 ms, and another sharp spike at about 430 ms. The sharp\n",
    "spikes at -40 ms and 450 ms look dissimilar from the other datapoints. \n",
    "\n",
    "Let's assume that we conclude that these spikes don't represent real activity. These datapoints are errors. For example, if this is physiological data, we might be confident that there's no way the physiological system could produce such large spikes, but we're aware that electrical noise in the nearby environment could do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to quantify how unusual those two datapoints are from the rest of the values is to calculate how many standard deviations those points are from the timeseries mean. A Z-score calculates this\n",
    "measure. In particular, a Z-score tells us how far a particular point is from the mean in units of standard\n",
    "deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Z-score formula is:\n",
    "    \n",
    "z = datapoint - mean / standard deviation\n",
    "\n",
    "Here are the two outliers (the very first value and the very last value in the sorted timeseeries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered = np.sort(rawdata)\n",
    "unfiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the z-score formula to calculate how many standard deviations each outlier value is from the\n",
    "mean.\n",
    "\n",
    "Given what you know about normal distributions and standard deviations, write a sentence about how\n",
    "likely it is that these two points are outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>DSP.04.G3.b) Correcting the data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variety of techniques are used to remove outliers. One option is to simply replace the outlier values\n",
    "with the value of the nearest point (or average of the nearest points).\n",
    "\n",
    "Give this a try. Create a corrected plot (as a friendly gesture, the code below will get you started).\n",
    "\n",
    "First identify the location of the points nearest to one of the outliers.\n",
    "\n",
    "Here is the position of the first outlier in the (original, unsorted) timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(rawdata == 217.431945)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the neighboring points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata[39]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the average value of the two nearby points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rawdata[37]+rawdata[39])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this replaces the outlier value with the new point (-0.725178 replaces the element in the 38th\n",
    "position in the list). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata[38] = -0.725178"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata[38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we just did was a digital filter, similar to but not the same as the moving average convolution technique that we discussed earlier. What's the difference? Previously, when we used a 3-item kernel, we set the weights to 1/3, 1/3, 1/3. Here, we also used a 3-item kernel, but we set the weights to 1/2, 0, 1/2. That is, we put a weight of 0 on the point we wanted to replace with the average of its 2 closest neighbors, and we put a weight of 1/2 on each of those 2 neighbors.\n",
    "\n",
    "Now, you fix the second outlier and then replot the data.\n",
    "\n",
    "For extra credit, modify the x axis so ticks are placed every 100 ms. It may not be as easy as it looks (and\n",
    "maybe it doesn't even look easy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>DSP.04.G3.c) Smoothing the Corrected Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's assume that you wanted to smooth the original timeseries, even before you noticed those 2 spikey error values. Let's check out what the impact is of our choosing to correct - or not correct - those 2 error values before we do our smoothing. You're going to use the moving average convolution technique to remove high-frequency activity.\n",
    "Apply the procedure to the corrected and uncorrected datasets (i.e., with and without outliers).\n",
    "Plot both filtered datasets and comment on whether it is better to remove outliers before applying the\n",
    "filter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
